{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef38763",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d0a9663",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class ProteinDataProcessor:\n",
    "    def __init__(self, data_dir=\"./data\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.pdb_structures = {}\n",
    "        self.binding_pockets = {}\n",
    "        self.pocket_descriptors = {}\n",
    "        self.output_df = pd.DataFrame()\n",
    "        \n",
    "    def prepare(self, system_type=\"all\", output_dir=\"./processed_data\"):\n",
    "        \"\"\"\n",
    "        Process and clean protein-protein interaction and ligand binding data.\n",
    "        \n",
    "        Args:\n",
    "            system_type: 'HD' (heterodimer), 'PL' (protein-ligand), or 'all'\n",
    "            output_dir: Directory to save processed data\n",
    "        \"\"\"\n",
    "        self._load_csv_annotations()\n",
    "        self._process_pdb_structures()\n",
    "        self._extract_pocket_features()\n",
    "        self._create_unified_dataset(system_type)\n",
    "        self._save_processed_data(output_dir)\n",
    "        \n",
    "    def _load_csv_annotations(self):\n",
    "        \"\"\"Load and process CSV annotation files\"\"\"\n",
    "        csv_patterns = {\n",
    "            'HD_orthosteric': 'HD_part8_20230317_matrix_orthosteric.csv',\n",
    "            'HD_complete': 'HD_part8_20230317PDBe_orthosteric__complete.csv',\n",
    "            'PL_allosteric': 'PL_part8_20230317_matrix_liganded_allosteric.csv',\n",
    "            'PL_allosteric_complete': 'PL_part8_20230317PDBe_allosteric__complete.csv',\n",
    "            'PL_orthosteric_comp': 'PL_part8_20230317_matrix_liganded_orthosteric_competitive.csv',\n",
    "            'PL_orthosteric_comp_complete': 'PL_part8_20230317PDBe_orthosteric_competitive__complete.csv',\n",
    "            'PL_orthosteric_noncomp': 'PL_part8_20230317_matrix_liganded_orthosteric_noncompetitive.csv',\n",
    "            'PL_orthosteric_noncomp_complete': 'PL_part8_20230317PDBe_orthosteric_noncompetitive__complete.csv'\n",
    "        }\n",
    "        \n",
    "        self.csv_data = {}\n",
    "        for key, filename in csv_patterns.items():\n",
    "            filepath = self.data_dir / filename\n",
    "            if filepath.exists():\n",
    "                print(f\"Loading {filename}\")\n",
    "                self.csv_data[key] = pd.read_csv(filepath)\n",
    "            else:\n",
    "                print(f\"Warning: {filename} not found\")\n",
    "    \n",
    "    def _process_pdb_structures(self):\n",
    "        \"\"\"Process PDB structure files and extract metadata\"\"\"\n",
    "        pdb_files = list(self.data_dir.glob(\"**/*.pdb\"))\n",
    "        mol2_files = list(self.data_dir.glob(\"**/*.mol2\"))\n",
    "        \n",
    "        for pdb_file in pdb_files:\n",
    "            pdb_info = self._parse_pdb_filename(pdb_file.name)\n",
    "            if pdb_info:\n",
    "                self.pdb_structures[pdb_file.stem] = {\n",
    "                    'file_path': pdb_file,\n",
    "                    'metadata': pdb_info\n",
    "                }\n",
    "        \n",
    "        for mol2_file in mol2_files:\n",
    "            pocket_info = self._parse_pocket_filename(mol2_file.name)\n",
    "            if pocket_info:\n",
    "                self.binding_pockets[mol2_file.stem] = {\n",
    "                    'file_path': mol2_file,\n",
    "                    'metadata': pocket_info\n",
    "                }\n",
    "    \n",
    "    def _parse_pdb_filename(self, filename):\n",
    "        \"\"\"Parse PDB filename to extract structure information\"\"\"\n",
    "        patterns = [\n",
    "            r'(\\w+)--(\\w+)--(\\w+)--(\\w+)\\.pdb',  # heterodimer complex\n",
    "            r'(\\w+)--(\\w)--(\\w+)__Repair-H\\.pdb',  # repaired chain\n",
    "            r'(\\w+)--(\\w)--(\\w+)\\.pdb',  # single chain\n",
    "            r'pdb(\\w+)\\.ent'  # raw PDB\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.match(pattern, filename)\n",
    "            if match:\n",
    "                groups = match.groups()\n",
    "                if len(groups) == 4:\n",
    "                    return {\n",
    "                        'pdb_code': groups[0],\n",
    "                        'chain1': groups[1],\n",
    "                        'uniprot1': groups[2],\n",
    "                        'chain2_or_ligand': groups[3],\n",
    "                        'type': 'heterodimer_complex'\n",
    "                    }\n",
    "                elif len(groups) == 3:\n",
    "                    return {\n",
    "                        'pdb_code': groups[0],\n",
    "                        'chain': groups[1],\n",
    "                        'uniprot': groups[2],\n",
    "                        'type': 'single_chain'\n",
    "                    }\n",
    "                elif len(groups) == 1:\n",
    "                    return {\n",
    "                        'pdb_code': groups[0],\n",
    "                        'type': 'raw_pdb'\n",
    "                    }\n",
    "        return None\n",
    "    \n",
    "    def _parse_pocket_filename(self, filename):\n",
    "        \"\"\"Parse pocket filename to extract cavity information\"\"\"\n",
    "        pattern = r'(\\w+)-(\\w+)-(\\w+)-?(\\w+)?-?(\\w+)?_CAVITY_N(\\d+)_ALL_(.+)\\.mol2'\n",
    "        match = re.match(pattern, filename)\n",
    "        \n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            return {\n",
    "                'pdb_code': groups[0],\n",
    "                'chain': groups[1],\n",
    "                'uniprot': groups[2],\n",
    "                'ligand': groups[3] if groups[3] else None,\n",
    "                'ligand_num': groups[4] if groups[4] else None,\n",
    "                'cavity_num': groups[5],\n",
    "                'pocket_type': groups[6]\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def _extract_pocket_features(self):\n",
    "        \"\"\"Extract and standardize pocket features from CSV data\"\"\"\n",
    "        all_features = []\n",
    "        \n",
    "        for dataset_name, df in self.csv_data.items():\n",
    "            if df is not None and not df.empty:\n",
    "                df_copy = df.copy()\n",
    "                df_copy['dataset_source'] = dataset_name\n",
    "                df_copy['system_type'] = 'HD' if 'HD' in dataset_name else 'PL'\n",
    "                \n",
    "                # Parse cavity identifier\n",
    "                if 'Cavity' in df_copy.columns:\n",
    "                    cavity_info = df_copy['Cavity'].apply(self._parse_cavity_identifier)\n",
    "                    for key in ['pdb_code', 'chain', 'uniprot', 'ligand_id', 'ligand_num', 'cavity_num', 'binding_type']:\n",
    "                        df_copy[key] = [info.get(key) for info in cavity_info]\n",
    "                \n",
    "                all_features.append(df_copy)\n",
    "        \n",
    "        if all_features:\n",
    "            self.pocket_descriptors = pd.concat(all_features, ignore_index=True, sort=False)\n",
    "        else:\n",
    "            self.pocket_descriptors = pd.DataFrame()\n",
    "    \n",
    "    def _parse_cavity_identifier(self, cavity_str):\n",
    "        \"\"\"Parse cavity identifier string\"\"\"\n",
    "        pattern = r'(\\w+)-(\\w+)-(\\w+)-?(\\w+)?-?(\\w+)?_CAVITY_N(\\d+)_(.+)'\n",
    "        match = re.match(pattern, str(cavity_str))\n",
    "        \n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            return {\n",
    "                'pdb_code': groups[0],\n",
    "                'chain': groups[1],\n",
    "                'uniprot': groups[2],\n",
    "                'ligand_id': groups[3] if groups[3] else None,\n",
    "                'ligand_num': groups[4] if groups[4] else None,\n",
    "                'cavity_num': groups[5],\n",
    "                'binding_type': groups[6]\n",
    "            }\n",
    "        return {}\n",
    "    \n",
    "    def _create_unified_dataset(self, system_type):\n",
    "        \"\"\"Create unified dataset with standardized features\"\"\"\n",
    "        if self.pocket_descriptors.empty:\n",
    "            print(\"No pocket descriptor data available\")\n",
    "            return\n",
    "        \n",
    "        # Filter by system type if specified\n",
    "        if system_type != \"all\":\n",
    "            df = self.pocket_descriptors[self.pocket_descriptors['system_type'] == system_type].copy()\n",
    "        else:\n",
    "            df = self.pocket_descriptors.copy()\n",
    "        \n",
    "        # Standardize geometric features\n",
    "        geometric_features = [\n",
    "            'Volume', 'PMI1', 'PMI2', 'PMI3', 'NPR1', 'NPR2', \n",
    "            'Rgyr', 'Asphericity', 'SpherocityIndex', 'Eccentricity', 'InertialShapeFactor'\n",
    "        ]\n",
    "        \n",
    "        # Standardize polarity features\n",
    "        polarity_features = ['CZ', 'CA', 'O', 'OD1', 'OG', 'N', 'NZ', 'DU']\n",
    "        \n",
    "        # Create feature groups\n",
    "        for feature in geometric_features:\n",
    "            if feature in df.columns:\n",
    "                df[f'{feature}_normalized'] = (df[feature] - df[feature].mean()) / df[feature].std()\n",
    "        \n",
    "        # Add derived features\n",
    "        if all(col in df.columns for col in ['PMI1', 'PMI2', 'PMI3']):\n",
    "            df['shape_anisotropy'] = (df['PMI1'] - df['PMI2']) / (df['PMI1'] + df['PMI2'] + df['PMI3'])\n",
    "        \n",
    "        # Categorize binding types\n",
    "        df['binding_category'] = df['binding_type'].apply(self._categorize_binding_type)\n",
    "        \n",
    "        # Add sequence-based features if available\n",
    "        if 'pfam_accession' in df.columns:\n",
    "            df['has_pfam'] = ~df['pfam_accession'].isna()\n",
    "        \n",
    "        if 'cath' in df.columns:\n",
    "            df['has_cath'] = ~df['cath'].isna()\n",
    "        \n",
    "        self.output_df = df\n",
    "    \n",
    "    def _categorize_binding_type(self, binding_type):\n",
    "        \"\"\"Categorize binding types into main categories\"\"\"\n",
    "        if pd.isna(binding_type):\n",
    "            return 'unknown'\n",
    "        \n",
    "        binding_type = str(binding_type).lower()\n",
    "        \n",
    "        if 'orthosteric' in binding_type:\n",
    "            if 'competitive' in binding_type:\n",
    "                return 'orthosteric_competitive'\n",
    "            elif 'noncompetitive' in binding_type:\n",
    "                return 'orthosteric_noncompetitive'\n",
    "            else:\n",
    "                return 'orthosteric'\n",
    "        elif 'allosteric' in binding_type:\n",
    "            return 'allosteric'\n",
    "        elif 'nonorthosteric' in binding_type:\n",
    "            return 'nonorthosteric'\n",
    "        else:\n",
    "            return 'other'\n",
    "    \n",
    "    def _save_processed_data(self, output_dir):\n",
    "        \"\"\"Save processed data to files\"\"\"\n",
    "        output_path = Path(output_dir)\n",
    "        output_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if not self.output_df.empty:\n",
    "            # Save main dataset\n",
    "            main_file = output_path / \"protein_pockets_processed.csv\"\n",
    "            print(f\"Saving processed data to: {main_file}\")\n",
    "            self.output_df.to_csv(main_file, index=False)\n",
    "            \n",
    "            # Save system-specific datasets\n",
    "            for system_type in ['HD', 'PL']:\n",
    "                system_df = self.output_df[self.output_df['system_type'] == system_type]\n",
    "                if not system_df.empty:\n",
    "                    system_file = output_path / f\"protein_pockets_{system_type.lower()}.csv\"\n",
    "                    print(f\"Saving {system_type} data to: {system_file}\")\n",
    "                    system_df.to_csv(system_file, index=False)\n",
    "            \n",
    "            # Save binding type specific datasets\n",
    "            for binding_cat in self.output_df['binding_category'].unique():\n",
    "                if pd.notna(binding_cat):\n",
    "                    binding_df = self.output_df[self.output_df['binding_category'] == binding_cat]\n",
    "                    binding_file = output_path / f\"protein_pockets_{binding_cat}.csv\"\n",
    "                    print(f\"Saving {binding_cat} data to: {binding_file}\")\n",
    "                    binding_df.to_csv(binding_file, index=False)\n",
    "            \n",
    "            # Generate summary statistics\n",
    "            self._generate_summary_report(output_path)\n",
    "        else:\n",
    "            print(\"No data to save\")\n",
    "    \n",
    "    def _generate_summary_report(self, output_path):\n",
    "        \"\"\"Generate summary statistics report\"\"\"\n",
    "        summary_file = output_path / \"processing_summary.txt\"\n",
    "        \n",
    "        with open(summary_file, 'w') as f:\n",
    "            f.write(\"Protein-Protein Interaction and Ligand Binding Dataset Processing Summary\\n\")\n",
    "            f.write(\"=\" * 70 + \"\\n\\n\")\n",
    "            \n",
    "            f.write(f\"Total processed entries: {len(self.output_df)}\\n\")\n",
    "            f.write(f\"System type distribution:\\n\")\n",
    "            for system_type, count in self.output_df['system_type'].value_counts().items():\n",
    "                f.write(f\"  {system_type}: {count}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nBinding category distribution:\\n\")\n",
    "            for binding_cat, count in self.output_df['binding_category'].value_counts().items():\n",
    "                f.write(f\"  {binding_cat}: {count}\\n\")\n",
    "            \n",
    "            f.write(f\"\\nUnique PDB codes: {self.output_df['pdb_code'].nunique()}\\n\")\n",
    "            f.write(f\"Unique UniProt IDs: {self.output_df['uniprot'].nunique()}\\n\")\n",
    "            \n",
    "            if 'Volume' in self.output_df.columns:\n",
    "                f.write(f\"\\nPocket volume statistics:\\n\")\n",
    "                f.write(f\"  Mean: {self.output_df['Volume'].mean():.2f}\\n\")\n",
    "                f.write(f\"  Std: {self.output_df['Volume'].std():.2f}\\n\")\n",
    "                f.write(f\"  Min: {self.output_df['Volume'].min():.2f}\\n\")\n",
    "                f.write(f\"  Max: {self.output_df['Volume'].max():.2f}\\n\")\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    data_dir = sys.argv[1] if len(sys.argv) > 1 else \"./data\"\n",
    "    system_type = sys.argv[2] if len(sys.argv) > 2 else \"all\"\n",
    "    \n",
    "    processor = ProteinDataProcessor(data_dir)\n",
    "    processor.prepare(system_type)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sfldf46a8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# GPU acceleration imports\n",
    "try:\n",
    "    import torch\n",
    "    import torch.nn.functional as F\n",
    "    HAS_TORCH = True\n",
    "    print(f\"PyTorch available: {torch.__version__}\")\n",
    "    \n",
    "    # Check for Metal Performance Shaders (macOS)\n",
    "    if torch.backends.mps.is_available():\n",
    "        DEVICE = torch.device(\"mps\")\n",
    "        print(\"Using Apple Metal Performance Shaders\")\n",
    "    # Check for CUDA (NVIDIA)\n",
    "    elif torch.cuda.is_available():\n",
    "        DEVICE = torch.device(\"cuda\")\n",
    "        print(f\"Using NVIDIA CUDA: {torch.cuda.get_device_name()}\")\n",
    "    else:\n",
    "        DEVICE = torch.device(\"cpu\")\n",
    "        print(\"Using CPU - consider installing CUDA/MPS support\")\n",
    "except ImportError:\n",
    "    HAS_TORCH = False\n",
    "    DEVICE = None\n",
    "    print(\"PyTorch not available - using CPU-only processing\")\n",
    "\n",
    "# Spark imports for large-scale processing\n",
    "try:\n",
    "    from pyspark.sql import SparkSession\n",
    "    from pyspark.sql.functions import col, count, avg, stddev, min as spark_min, max as spark_max\n",
    "    from pyspark.sql.types import StructType, StructField, StringType, DoubleType, IntegerType\n",
    "    HAS_SPARK = True\n",
    "    print(\"PySpark available\")\n",
    "except ImportError:\n",
    "    HAS_SPARK = False\n",
    "    print(\"PySpark not available - using pandas for processing\")\n",
    "\n",
    "class AcceleratedProteinDataProcessor:\n",
    "    def __init__(self, data_dir=\"/Users/priyanshudey/Code/Qunatum/ippidb-pdb-analyses-042023-zenodo\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.device = DEVICE\n",
    "        self.use_gpu = HAS_TORCH and DEVICE != torch.device(\"cpu\")\n",
    "        self.use_spark = HAS_SPARK\n",
    "        self.spark = None\n",
    "        \n",
    "        # Initialize Spark if available\n",
    "        if self.use_spark:\n",
    "            self.spark = SparkSession.builder \\\n",
    "                .appName(\"ProteinDataScoping\") \\\n",
    "                .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "                .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "                .getOrCreate()\n",
    "            print(f\"Spark initialized with {self.spark.sparkContext.defaultParallelism} cores\")\n",
    "        \n",
    "        self.csv_files = {}\n",
    "        self.pdb_structures = {}\n",
    "        self.binding_pockets = {}\n",
    "        self.summary_stats = {}\n",
    "        \n",
    "    def initial_data_scoping(self, sample_size=None, parallel_jobs=None):\n",
    "        \"\"\"\n",
    "        Perform initial data scoping with GPU/Metal acceleration\n",
    "        \"\"\"\n",
    "        print(\"=== Initial Data Scoping with Acceleration ===\")\n",
    "        start_time = time.time()\n",
    "        \n",
    "        if parallel_jobs is None:\n",
    "            parallel_jobs = min(mp.cpu_count(), 8)\n",
    "            \n",
    "        # Step 1: Discover all data files\n",
    "        print(\"1. Discovering data files...\")\n",
    "        self._discover_files_parallel(parallel_jobs)\n",
    "        \n",
    "        # Step 2: Load and analyze CSV files\n",
    "        print(\"2. Loading and analyzing CSV files...\")\n",
    "        self._load_csv_files_accelerated(sample_size)\n",
    "        \n",
    "        # Step 3: Analyze PDB structures\n",
    "        print(\"3. Analyzing PDB structures...\")\n",
    "        self._analyze_structures_parallel(parallel_jobs)\n",
    "        \n",
    "        # Step 4: Generate comprehensive statistics\n",
    "        print(\"4. Generating statistics...\")\n",
    "        self._generate_accelerated_stats()\n",
    "        \n",
    "        # Step 5: Create data summary report\n",
    "        print(\"5. Creating summary report...\")\n",
    "        self._create_scoping_report()\n",
    "        \n",
    "        total_time = time.time() - start_time\n",
    "        print(f\"\\\\nData scoping completed in {total_time:.2f} seconds\")\n",
    "        \n",
    "        return self.summary_stats\n",
    "    \n",
    "    def _discover_files_parallel(self, n_jobs):\n",
    "        \"\"\"Parallel file discovery\"\"\"\n",
    "        print(f\"Scanning directory with {n_jobs} parallel workers...\")\n",
    "        \n",
    "        def scan_directory(root_dir):\n",
    "            results = {\n",
    "                'csv_files': [],\n",
    "                'pdb_files': [],\n",
    "                'mol2_files': [],\n",
    "                'other_files': []\n",
    "            }\n",
    "            \n",
    "            for file_path in root_dir.rglob(\"*\"):\n",
    "                if file_path.is_file():\n",
    "                    suffix = file_path.suffix.lower()\n",
    "                    if suffix == '.csv':\n",
    "                        results['csv_files'].append(file_path)\n",
    "                    elif suffix == '.pdb':\n",
    "                        results['pdb_files'].append(file_path)\n",
    "                    elif suffix == '.mol2':\n",
    "                        results['mol2_files'].append(file_path)\n",
    "                    else:\n",
    "                        results['other_files'].append(file_path)\n",
    "            \n",
    "            return results\n",
    "        \n",
    "        # Split directory scanning across workers\n",
    "        subdirs = [d for d in self.data_dir.iterdir() if d.is_dir()]\n",
    "        if len(subdirs) > n_jobs:\n",
    "            chunk_size = len(subdirs) // n_jobs\n",
    "            dir_chunks = [subdirs[i:i+chunk_size] for i in range(0, len(subdirs), chunk_size)]\n",
    "        else:\n",
    "            dir_chunks = [[d] for d in subdirs]\n",
    "        \n",
    "        with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "            futures = []\n",
    "            for chunk in dir_chunks:\n",
    "                for subdir in chunk:\n",
    "                    futures.append(executor.submit(scan_directory, subdir))\n",
    "            \n",
    "            # Combine results\n",
    "            all_results = {'csv_files': [], 'pdb_files': [], 'mol2_files': [], 'other_files': []}\n",
    "            for future in futures:\n",
    "                result = future.result()\n",
    "                for key in all_results:\n",
    "                    all_results[key].extend(result[key])\n",
    "        \n",
    "        self.file_inventory = all_results\n",
    "        print(f\"Found {len(all_results['csv_files'])} CSV files\")\n",
    "        print(f\"Found {len(all_results['pdb_files'])} PDB files\") \n",
    "        print(f\"Found {len(all_results['mol2_files'])} MOL2 files\")\n",
    "        \n",
    "    def _load_csv_files_accelerated(self, sample_size):\n",
    "        \"\"\"Load CSV files with acceleration and sampling\"\"\"\n",
    "        csv_patterns = {\n",
    "            'HD_orthosteric': 'HD_part8_20230317_matrix_orthosteric.csv',\n",
    "            'HD_complete': 'HD_part8_20230317_matrix_orthosteric__complete.csv',\n",
    "            'PL_allosteric': 'PL_part8_20230317_matrix_liganded_allosteric.csv',\n",
    "            'PL_allosteric_complete': 'PL_part8_20230317_matrix_liganded_allosteric__complete.csv',\n",
    "            'PL_orthosteric_comp': 'PL_part8_20230317_matrix_liganded_orthosteric_competitive.csv',\n",
    "            'PL_orthosteric_comp_complete': 'PL_part8_20230317_matrix_liganded_orthosteric_competitive__complete.csv',\n",
    "            'PL_orthosteric_noncomp': 'PL_part8_20230317_matrix_liganded_orthosteric_noncompetitive.csv',\n",
    "            'PL_orthosteric_noncomp_complete': 'PL_part8_20230317_matrix_liganded_orthosteric_noncompetitive__complete.csv'\n",
    "        }\n",
    "        \n",
    "        def load_and_sample_csv(file_path, sample_size=None):\n",
    "            try:\n",
    "                if sample_size:\n",
    "                    # Read just the header first to get column count\n",
    "                    header = pd.read_csv(file_path, nrows=0)\n",
    "                    n_rows = sum(1 for _ in open(file_path)) - 1  # subtract header\n",
    "                    \n",
    "                    if n_rows > sample_size:\n",
    "                        # Random sampling\n",
    "                        skip_rows = np.random.choice(range(1, n_rows + 1), \n",
    "                                                   n_rows - sample_size, replace=False)\n",
    "                        df = pd.read_csv(file_path, skiprows=skip_rows)\n",
    "                    else:\n",
    "                        df = pd.read_csv(file_path)\n",
    "                else:\n",
    "                    df = pd.read_csv(file_path)\n",
    "                \n",
    "                return df, len(df), df.columns.tolist()\n",
    "            except Exception as e:\n",
    "                print(f\"Error loading {file_path}: {e}\")\n",
    "                return None, 0, []\n",
    "        \n",
    "        # Load known CSV files\n",
    "        loaded_data = {}\n",
    "        for key, filename in csv_patterns.items():\n",
    "            file_path = self.data_dir / filename\n",
    "            if file_path.exists():\n",
    "                print(f\"Loading {filename}...\")\n",
    "                df, n_rows, columns = load_and_sample_csv(file_path, sample_size)\n",
    "                if df is not None:\n",
    "                    loaded_data[key] = {\n",
    "                        'data': df,\n",
    "                        'n_rows': n_rows,\n",
    "                        'n_cols': len(columns),\n",
    "                        'columns': columns,\n",
    "                        'file_path': file_path\n",
    "                    }\n",
    "        \n",
    "        self.csv_files = loaded_data\n",
    "        \n",
    "        # If using Spark, also load into Spark DataFrames for large-scale processing\n",
    "        if self.use_spark and loaded_data:\n",
    "            print(\"Loading data into Spark...\")\n",
    "            self.spark_dfs = {}\n",
    "            for key, data_info in loaded_data.items():\n",
    "                spark_df = self.spark.createDataFrame(data_info['data'])\n",
    "                self.spark_dfs[key] = spark_df\n",
    "                print(f\"  {key}: {spark_df.count()} rows, {len(spark_df.columns)} columns\")\n",
    "    \n",
    "    def _analyze_structures_parallel(self, n_jobs):\n",
    "        \"\"\"Parallel analysis of protein structures\"\"\"\n",
    "        pdb_files = self.file_inventory.get('pdb_files', [])\n",
    "        mol2_files = self.file_inventory.get('mol2_files', [])\n",
    "        \n",
    "        def analyze_pdb_file(pdb_path):\n",
    "            \"\"\"Extract basic info from PDB file\"\"\"\n",
    "            try:\n",
    "                info = self._parse_pdb_filename(pdb_path.name)\n",
    "                if info:\n",
    "                    # Quick file size and basic stats\n",
    "                    file_size = pdb_path.stat().st_size\n",
    "                    \n",
    "                    # Count lines and atoms (quick scan)\n",
    "                    with open(pdb_path, 'r') as f:\n",
    "                        lines = f.readlines()\n",
    "                    \n",
    "                    atom_lines = [l for l in lines if l.startswith('ATOM')]\n",
    "                    hetatm_lines = [l for l in lines if l.startswith('HETATM')]\n",
    "                    \n",
    "                    info.update({\n",
    "                        'file_size': file_size,\n",
    "                        'total_lines': len(lines),\n",
    "                        'atom_count': len(atom_lines),\n",
    "                        'hetatm_count': len(hetatm_lines),\n",
    "                        'file_path': str(pdb_path)\n",
    "                    })\n",
    "                \n",
    "                return info\n",
    "            except Exception as e:\n",
    "                return {'error': str(e), 'file_path': str(pdb_path)}\n",
    "        \n",
    "        def analyze_mol2_file(mol2_path):\n",
    "            \"\"\"Extract basic info from MOL2 file\"\"\"\n",
    "            try:\n",
    "                info = self._parse_pocket_filename(mol2_path.name)\n",
    "                if info:\n",
    "                    file_size = mol2_path.stat().st_size\n",
    "                    info.update({\n",
    "                        'file_size': file_size,\n",
    "                        'file_path': str(mol2_path)\n",
    "                    })\n",
    "                return info\n",
    "            except Exception as e:\n",
    "                return {'error': str(e), 'file_path': str(mol2_path)}\n",
    "        \n",
    "        print(f\"Analyzing {len(pdb_files)} PDB files with {n_jobs} workers...\")\n",
    "        with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "            pdb_results = list(executor.map(analyze_pdb_file, pdb_files))\n",
    "        \n",
    "        print(f\"Analyzing {len(mol2_files)} MOL2 files with {n_jobs} workers...\")\n",
    "        with ProcessPoolExecutor(max_workers=n_jobs) as executor:\n",
    "            mol2_results = list(executor.map(analyze_mol2_file, mol2_files))\n",
    "        \n",
    "        self.pdb_structures = {f\"pdb_{i}\": result for i, result in enumerate(pdb_results) if result}\n",
    "        self.binding_pockets = {f\"pocket_{i}\": result for i, result in enumerate(mol2_results) if result}\n",
    "        \n",
    "        print(f\"Analyzed {len(self.pdb_structures)} PDB structures\")\n",
    "        print(f\"Analyzed {len(self.binding_pockets)} binding pockets\")\n",
    "    \n",
    "    def _generate_accelerated_stats(self):\n",
    "        \"\"\"Generate statistics using GPU acceleration where possible\"\"\"\n",
    "        stats = {}\n",
    "        \n",
    "        # CSV file statistics\n",
    "        if self.csv_files:\n",
    "            csv_stats = {}\n",
    "            for key, data_info in self.csv_files.items():\n",
    "                df = data_info['data']\n",
    "                \n",
    "                # Basic stats\n",
    "                basic_stats = {\n",
    "                    'n_rows': len(df),\n",
    "                    'n_cols': len(df.columns),\n",
    "                    'memory_usage': df.memory_usage().sum(),\n",
    "                    'columns': df.columns.tolist()\n",
    "                }\n",
    "                \n",
    "                # Numeric column analysis\n",
    "                numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "                if len(numeric_cols) > 0 and self.use_gpu:\n",
    "                    # GPU-accelerated statistics\n",
    "                    numeric_data = df[numeric_cols].values\n",
    "                    if numeric_data.size > 0:\n",
    "                        tensor_data = torch.tensor(numeric_data, dtype=torch.float32).to(self.device)\n",
    "                        \n",
    "                        gpu_stats = {\n",
    "                            'mean': tensor_data.mean(dim=0).cpu().numpy().tolist(),\n",
    "                            'std': tensor_data.std(dim=0).cpu().numpy().tolist(),\n",
    "                            'min': tensor_data.min(dim=0)[0].cpu().numpy().tolist(),\n",
    "                            'max': tensor_data.max(dim=0)[0].cpu().numpy().tolist()\n",
    "                        }\n",
    "                        \n",
    "                        basic_stats['numeric_stats'] = dict(zip(numeric_cols, \n",
    "                                                              zip(gpu_stats['mean'], gpu_stats['std'],\n",
    "                                                                  gpu_stats['min'], gpu_stats['max'])))\n",
    "                elif len(numeric_cols) > 0:\n",
    "                    # Fallback to pandas\n",
    "                    basic_stats['numeric_stats'] = df[numeric_cols].describe().to_dict()\n",
    "                \n",
    "                # Categorical analysis\n",
    "                cat_cols = df.select_dtypes(include=['object']).columns\n",
    "                if len(cat_cols) > 0:\n",
    "                    cat_stats = {}\n",
    "                    for col in cat_cols[:5]:  # Limit to first 5 categorical columns\n",
    "                        cat_stats[col] = {\n",
    "                            'unique_count': df[col].nunique(),\n",
    "                            'top_values': df[col].value_counts().head().to_dict()\n",
    "                        }\n",
    "                    basic_stats['categorical_stats'] = cat_stats\n",
    "                \n",
    "                csv_stats[key] = basic_stats\n",
    "            \n",
    "            stats['csv_files'] = csv_stats\n",
    "        \n",
    "        # Structure file statistics\n",
    "        if self.pdb_structures:\n",
    "            pdb_stats = {\n",
    "                'total_files': len(self.pdb_structures),\n",
    "                'total_atoms': sum(s.get('atom_count', 0) for s in self.pdb_structures.values() if 'atom_count' in s),\n",
    "                'total_size': sum(s.get('file_size', 0) for s in self.pdb_structures.values() if 'file_size' in s)\n",
    "            }\n",
    "            \n",
    "            # PDB type distribution\n",
    "            pdb_types = [s.get('type') for s in self.pdb_structures.values() if 'type' in s]\n",
    "            if pdb_types:\n",
    "                type_counts = pd.Series(pdb_types).value_counts().to_dict()\n",
    "                pdb_stats['type_distribution'] = type_counts\n",
    "            \n",
    "            stats['pdb_structures'] = pdb_stats\n",
    "        \n",
    "        if self.binding_pockets:\n",
    "            pocket_stats = {\n",
    "                'total_files': len(self.binding_pockets),\n",
    "                'total_size': sum(s.get('file_size', 0) for s in self.binding_pockets.values() if 'file_size' in s)\n",
    "            }\n",
    "            stats['binding_pockets'] = pocket_stats\n",
    "        \n",
    "        self.summary_stats = stats\n",
    "        return stats\n",
    "    \n",
    "    def _parse_pdb_filename(self, filename):\n",
    "        \"\"\"Parse PDB filename patterns\"\"\"\n",
    "        patterns = [\n",
    "            r'(\\\\w+)--(\\\\w+)--(\\\\w+)--(\\\\w+)\\\\.pdb',  # heterodimer complex\n",
    "            r'(\\\\w+)--(\\\\w)--(\\\\w+)__Repair-H\\\\.pdb',  # repaired chain\n",
    "            r'(\\\\w+)--(\\\\w)--(\\\\w+)\\\\.pdb',  # single chain\n",
    "            r'pdb(\\\\w+)\\\\.ent'  # raw PDB\n",
    "        ]\n",
    "        \n",
    "        for pattern in patterns:\n",
    "            match = re.match(pattern, filename)\n",
    "            if match:\n",
    "                groups = match.groups()\n",
    "                if len(groups) == 4:\n",
    "                    return {\n",
    "                        'pdb_code': groups[0],\n",
    "                        'chain1': groups[1], \n",
    "                        'uniprot1': groups[2],\n",
    "                        'chain2_or_ligand': groups[3],\n",
    "                        'type': 'heterodimer_complex'\n",
    "                    }\n",
    "                elif len(groups) == 3:\n",
    "                    return {\n",
    "                        'pdb_code': groups[0],\n",
    "                        'chain': groups[1],\n",
    "                        'uniprot': groups[2],\n",
    "                        'type': 'single_chain'\n",
    "                    }\n",
    "                elif len(groups) == 1:\n",
    "                    return {\n",
    "                        'pdb_code': groups[0],\n",
    "                        'type': 'raw_pdb'\n",
    "                    }\n",
    "        return None\n",
    "    \n",
    "    def _parse_pocket_filename(self, filename):\n",
    "        \"\"\"Parse pocket filename patterns\"\"\"\n",
    "        pattern = r'(\\\\w+)-(\\\\w+)-(\\\\w+)-?(\\\\w+)?-?(\\\\w+)?_CAVITY_N(\\\\d+)_ALL_(.+)\\\\.mol2'\n",
    "        match = re.match(pattern, filename)\n",
    "        \n",
    "        if match:\n",
    "            groups = match.groups()\n",
    "            return {\n",
    "                'pdb_code': groups[0],\n",
    "                'chain': groups[1],\n",
    "                'uniprot': groups[2],\n",
    "                'ligand': groups[3] if groups[3] else None,\n",
    "                'ligand_num': groups[4] if groups[4] else None,\n",
    "                'cavity_num': groups[5],\n",
    "                'pocket_type': groups[6]\n",
    "            }\n",
    "        return None\n",
    "    \n",
    "    def _create_scoping_report(self):\n",
    "        \"\"\"Create comprehensive scoping report\"\"\"\n",
    "        report_path = self.data_dir / \"data_scoping_report.txt\"\n",
    "        \n",
    "        with open(report_path, 'w') as f:\n",
    "            f.write(\"IPPIDB Protein Data Scoping Report\\\\n\")\n",
    "            f.write(\"=\" * 50 + \"\\\\n\\\\n\")\n",
    "            \n",
    "            # Hardware info\n",
    "            f.write(\"Hardware Configuration:\\\\n\")\n",
    "            f.write(f\"  GPU Acceleration: {'Yes' if self.use_gpu else 'No'}\\\\n\")\n",
    "            if self.use_gpu:\n",
    "                f.write(f\"  Device: {self.device}\\\\n\")\n",
    "            f.write(f\"  Spark Processing: {'Yes' if self.use_spark else 'No'}\\\\n\")\n",
    "            f.write(f\"  CPU Cores: {mp.cpu_count()}\\\\n\\\\n\")\n",
    "            \n",
    "            # File inventory\n",
    "            if hasattr(self, 'file_inventory'):\n",
    "                f.write(\"File Inventory:\\\\n\")\n",
    "                for file_type, files in self.file_inventory.items():\n",
    "                    f.write(f\"  {file_type}: {len(files)}\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "            \n",
    "            # CSV data summary\n",
    "            if 'csv_files' in self.summary_stats:\n",
    "                f.write(\"CSV Data Summary:\\\\n\")\n",
    "                for key, stats in self.summary_stats['csv_files'].items():\n",
    "                    f.write(f\"  {key}:\\\\n\")\n",
    "                    f.write(f\"    Rows: {stats['n_rows']:,}\\\\n\")\n",
    "                    f.write(f\"    Columns: {stats['n_cols']}\\\\n\")\n",
    "                    f.write(f\"    Memory: {stats['memory_usage']:,} bytes\\\\n\")\n",
    "                    \n",
    "                    if 'numeric_stats' in stats:\n",
    "                        f.write(f\"    Numeric columns: {len(stats['numeric_stats'])}\\\\n\")\n",
    "                    \n",
    "                    if 'categorical_stats' in stats:\n",
    "                        f.write(f\"    Categorical columns: {len(stats['categorical_stats'])}\\\\n\")\n",
    "                    f.write(\"\\\\n\")\n",
    "            \n",
    "            # Structure data summary\n",
    "            if 'pdb_structures' in self.summary_stats:\n",
    "                pdb_stats = self.summary_stats['pdb_structures']\n",
    "                f.write(\"PDB Structures Summary:\\\\n\")\n",
    "                f.write(f\"  Total files: {pdb_stats['total_files']:,}\\\\n\")\n",
    "                f.write(f\"  Total atoms: {pdb_stats['total_atoms']:,}\\\\n\")\n",
    "                f.write(f\"  Total size: {pdb_stats['total_size']:,} bytes\\\\n\")\n",
    "                \n",
    "                if 'type_distribution' in pdb_stats:\n",
    "                    f.write(\"  Type distribution:\\\\n\")\n",
    "                    for pdb_type, count in pdb_stats['type_distribution'].items():\n",
    "                        f.write(f\"    {pdb_type}: {count}\\\\n\")\n",
    "                f.write(\"\\\\n\")\n",
    "            \n",
    "            if 'binding_pockets' in self.summary_stats:\n",
    "                pocket_stats = self.summary_stats['binding_pockets']\n",
    "                f.write(\"Binding Pockets Summary:\\\\n\")\n",
    "                f.write(f\"  Total files: {pocket_stats['total_files']:,}\\\\n\")\n",
    "                f.write(f\"  Total size: {pocket_stats['total_size']:,} bytes\\\\n\\\\n\")\n",
    "        \n",
    "        print(f\"Scoping report saved to: {report_path}\")\n",
    "        return report_path\n",
    "    \n",
    "    def get_sample_data(self, dataset_key, n_samples=100):\n",
    "        \"\"\"Get sample data for exploration\"\"\"\n",
    "        if dataset_key in self.csv_files:\n",
    "            df = self.csv_files[dataset_key]['data']\n",
    "            return df.sample(min(n_samples, len(df)))\n",
    "        return None\n",
    "    \n",
    "    def cleanup(self):\n",
    "        \"\"\"Clean up resources\"\"\"\n",
    "        if self.spark:\n",
    "            self.spark.stop()\n",
    "            print(\"Spark session stopped\")\n",
    "\n",
    "# Initialize and run data scoping\n",
    "print(\"Initializing Accelerated Protein Data Processor...\")\n",
    "processor = AcceleratedProteinDataProcessor()\n",
    "\n",
    "# Run initial data scoping\n",
    "stats = processor.initial_data_scoping(sample_size=10000, parallel_jobs=6)\n",
    "\n",
    "print(\"\\\\n=== Data Scoping Results ===\")\n",
    "for key, value in stats.items():\n",
    "    print(f\"{key}: {len(value) if isinstance(value, dict) else value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ck0l1itj7s",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analysis functions with Spark integration\n",
    "class AdvancedProteinAnalysis:\n",
    "    def __init__(self, processor):\n",
    "        self.processor = processor\n",
    "        self.spark = processor.spark if processor.use_spark else None\n",
    "        \n",
    "    def large_scale_feature_analysis(self):\n",
    "        \"\"\"Perform large-scale feature analysis using Spark\"\"\"\n",
    "        if not self.spark:\n",
    "            print(\"Spark not available - using pandas fallback\")\n",
    "            return self._pandas_feature_analysis()\n",
    "        \n",
    "        print(\"=== Large-Scale Feature Analysis with Spark ===\")\n",
    "        results = {}\n",
    "        \n",
    "        for dataset_name, spark_df in self.processor.spark_dfs.items():\n",
    "            print(f\"Analyzing {dataset_name}...\")\n",
    "            \n",
    "            # Get numeric columns\n",
    "            numeric_cols = []\n",
    "            for field in spark_df.schema.fields:\n",
    "                if field.dataType.typeName() in ['double', 'float', 'integer', 'long']:\n",
    "                    numeric_cols.append(field.name)\n",
    "            \n",
    "            if numeric_cols:\n",
    "                # Spark aggregations for all numeric columns\n",
    "                agg_exprs = []\n",
    "                for col_name in numeric_cols:\n",
    "                    agg_exprs.extend([\n",
    "                        avg(col(col_name)).alias(f\"{col_name}_mean\"),\n",
    "                        stddev(col(col_name)).alias(f\"{col_name}_std\"),\n",
    "                        spark_min(col(col_name)).alias(f\"{col_name}_min\"),\n",
    "                        spark_max(col(col_name)).alias(f\"{col_name}_max\")\n",
    "                    ])\n",
    "                \n",
    "                stats_df = spark_df.agg(*agg_exprs)\n",
    "                stats_row = stats_df.collect()[0]\n",
    "                \n",
    "                # Organize results\n",
    "                feature_stats = {}\n",
    "                for col_name in numeric_cols:\n",
    "                    feature_stats[col_name] = {\n",
    "                        'mean': stats_row[f\"{col_name}_mean\"],\n",
    "                        'std': stats_row[f\"{col_name}_std\"],\n",
    "                        'min': stats_row[f\"{col_name}_min\"],\n",
    "                        'max': stats_row[f\"{col_name}_max\"]\n",
    "                    }\n",
    "                \n",
    "                results[dataset_name] = {\n",
    "                    'total_rows': spark_df.count(),\n",
    "                    'numeric_features': feature_stats,\n",
    "                    'feature_count': len(numeric_cols)\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def _pandas_feature_analysis(self):\n",
    "        \"\"\"Fallback pandas-based feature analysis\"\"\"\n",
    "        print(\"=== Feature Analysis with Pandas ===\")\n",
    "        results = {}\n",
    "        \n",
    "        for dataset_name, data_info in self.processor.csv_files.items():\n",
    "            df = data_info['data']\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            \n",
    "            if len(numeric_cols) > 0:\n",
    "                feature_stats = {}\n",
    "                for col in numeric_cols:\n",
    "                    feature_stats[col] = {\n",
    "                        'mean': df[col].mean(),\n",
    "                        'std': df[col].std(),\n",
    "                        'min': df[col].min(),\n",
    "                        'max': df[col].max()\n",
    "                    }\n",
    "                \n",
    "                results[dataset_name] = {\n",
    "                    'total_rows': len(df),\n",
    "                    'numeric_features': feature_stats,\n",
    "                    'feature_count': len(numeric_cols)\n",
    "                }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def gpu_accelerated_clustering_analysis(self, dataset_key, n_clusters=5, max_features=20):\n",
    "        \"\"\"GPU-accelerated clustering analysis of protein pocket features\"\"\"\n",
    "        if dataset_key not in self.processor.csv_files:\n",
    "            print(f\"Dataset {dataset_key} not found\")\n",
    "            return None\n",
    "        \n",
    "        df = self.processor.csv_files[dataset_key]['data']\n",
    "        numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        if len(numeric_cols) == 0:\n",
    "            print(\"No numeric features found for clustering\")\n",
    "            return None\n",
    "        \n",
    "        # Limit features for demonstration\n",
    "        selected_cols = numeric_cols[:max_features]\n",
    "        data = df[selected_cols].fillna(df[selected_cols].mean())\n",
    "        \n",
    "        print(f\"Performing clustering analysis on {len(selected_cols)} features...\")\n",
    "        \n",
    "        if self.processor.use_gpu and HAS_TORCH:\n",
    "            return self._gpu_clustering(data, n_clusters, selected_cols)\n",
    "        else:\n",
    "            return self._cpu_clustering(data, n_clusters, selected_cols)\n",
    "    \n",
    "    def _gpu_clustering(self, data, n_clusters, feature_names):\n",
    "        \"\"\"GPU-accelerated K-means clustering\"\"\"\n",
    "        print(\"Using GPU-accelerated clustering...\")\n",
    "        \n",
    "        # Convert to tensor and normalize\n",
    "        X = torch.tensor(data.values, dtype=torch.float32).to(self.processor.device)\n",
    "        X_normalized = F.normalize(X, p=2, dim=1)\n",
    "        \n",
    "        # Simple K-means implementation on GPU\n",
    "        n_samples, n_features = X_normalized.shape\n",
    "        \n",
    "        # Initialize centroids randomly\n",
    "        centroids = torch.randn(n_clusters, n_features).to(self.processor.device)\n",
    "        centroids = F.normalize(centroids, p=2, dim=1)\n",
    "        \n",
    "        for iteration in range(100):  # max iterations\n",
    "            # Compute distances to centroids\n",
    "            distances = torch.cdist(X_normalized, centroids)\n",
    "            cluster_assignments = torch.argmin(distances, dim=1)\n",
    "            \n",
    "            # Update centroids\n",
    "            new_centroids = torch.zeros_like(centroids)\n",
    "            for k in range(n_clusters):\n",
    "                mask = cluster_assignments == k\n",
    "                if mask.sum() > 0:\n",
    "                    new_centroids[k] = X_normalized[mask].mean(dim=0)\n",
    "                else:\n",
    "                    new_centroids[k] = centroids[k]  # Keep old centroid\n",
    "            \n",
    "            # Check for convergence\n",
    "            if torch.allclose(centroids, new_centroids, rtol=1e-4):\n",
    "                print(f\"Converged after {iteration+1} iterations\")\n",
    "                break\n",
    "                \n",
    "            centroids = new_centroids\n",
    "        \n",
    "        # Compute final assignments and statistics\n",
    "        distances = torch.cdist(X_normalized, centroids)\n",
    "        final_assignments = torch.argmin(distances, dim=1).cpu().numpy()\n",
    "        \n",
    "        # Cluster statistics\n",
    "        cluster_stats = {}\n",
    "        for k in range(n_clusters):\n",
    "            mask = final_assignments == k\n",
    "            cluster_size = mask.sum()\n",
    "            if cluster_size > 0:\n",
    "                cluster_data = data.iloc[mask]\n",
    "                cluster_stats[f'cluster_{k}'] = {\n",
    "                    'size': int(cluster_size),\n",
    "                    'percentage': float(cluster_size / len(data) * 100),\n",
    "                    'feature_means': cluster_data.mean().to_dict()\n",
    "                }\n",
    "        \n",
    "        return {\n",
    "            'method': 'gpu_kmeans',\n",
    "            'n_clusters': n_clusters,\n",
    "            'n_features': len(feature_names),\n",
    "            'feature_names': list(feature_names),\n",
    "            'cluster_assignments': final_assignments.tolist(),\n",
    "            'cluster_statistics': cluster_stats\n",
    "        }\n",
    "    \n",
    "    def _cpu_clustering(self, data, n_clusters, feature_names):\n",
    "        \"\"\"CPU-based clustering fallback\"\"\"\n",
    "        print(\"Using CPU-based clustering...\")\n",
    "        \n",
    "        try:\n",
    "            from sklearn.cluster import KMeans\n",
    "            from sklearn.preprocessing import StandardScaler\n",
    "            \n",
    "            # Normalize data\n",
    "            scaler = StandardScaler()\n",
    "            X_scaled = scaler.fit_transform(data)\n",
    "            \n",
    "            # K-means clustering\n",
    "            kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)\n",
    "            cluster_assignments = kmeans.fit_predict(X_scaled)\n",
    "            \n",
    "            # Cluster statistics\n",
    "            cluster_stats = {}\n",
    "            for k in range(n_clusters):\n",
    "                mask = cluster_assignments == k\n",
    "                cluster_size = mask.sum()\n",
    "                if cluster_size > 0:\n",
    "                    cluster_data = data.iloc[mask]\n",
    "                    cluster_stats[f'cluster_{k}'] = {\n",
    "                        'size': int(cluster_size),\n",
    "                        'percentage': float(cluster_size / len(data) * 100),\n",
    "                        'feature_means': cluster_data.mean().to_dict()\n",
    "                    }\n",
    "            \n",
    "            return {\n",
    "                'method': 'cpu_kmeans',\n",
    "                'n_clusters': n_clusters,\n",
    "                'n_features': len(feature_names),\n",
    "                'feature_names': list(feature_names),\n",
    "                'cluster_assignments': cluster_assignments.tolist(),\n",
    "                'cluster_statistics': cluster_stats\n",
    "            }\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"scikit-learn not available for CPU clustering\")\n",
    "            return None\n",
    "    \n",
    "    def generate_data_quality_report(self):\n",
    "        \"\"\"Generate comprehensive data quality report\"\"\"\n",
    "        print(\"=== Generating Data Quality Report ===\")\n",
    "        \n",
    "        quality_report = {}\n",
    "        \n",
    "        for dataset_name, data_info in self.processor.csv_files.items():\n",
    "            df = data_info['data']\n",
    "            \n",
    "            # Basic quality metrics\n",
    "            total_cells = df.size\n",
    "            missing_cells = df.isnull().sum().sum()\n",
    "            missing_percentage = (missing_cells / total_cells) * 100\n",
    "            \n",
    "            # Column-wise missing data\n",
    "            missing_by_column = df.isnull().sum()\n",
    "            columns_with_missing = missing_by_column[missing_by_column > 0]\n",
    "            \n",
    "            # Duplicate rows\n",
    "            duplicate_rows = df.duplicated().sum()\n",
    "            \n",
    "            # Data type distribution\n",
    "            dtype_counts = df.dtypes.value_counts().to_dict()\n",
    "            \n",
    "            # Numeric column quality\n",
    "            numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "            numeric_quality = {}\n",
    "            \n",
    "            for col in numeric_cols:\n",
    "                col_data = df[col].dropna()\n",
    "                if len(col_data) > 0:\n",
    "                    numeric_quality[col] = {\n",
    "                        'missing_count': df[col].isnull().sum(),\n",
    "                        'missing_percentage': (df[col].isnull().sum() / len(df)) * 100,\n",
    "                        'infinite_values': np.isinf(col_data).sum(),\n",
    "                        'zero_values': (col_data == 0).sum(),\n",
    "                        'negative_values': (col_data < 0).sum(),\n",
    "                        'outliers_iqr': self._count_outliers_iqr(col_data)\n",
    "                    }\n",
    "            \n",
    "            quality_report[dataset_name] = {\n",
    "                'total_rows': len(df),\n",
    "                'total_columns': len(df.columns),\n",
    "                'total_cells': total_cells,\n",
    "                'missing_cells': missing_cells,\n",
    "                'missing_percentage': missing_percentage,\n",
    "                'duplicate_rows': duplicate_rows,\n",
    "                'columns_with_missing': columns_with_missing.to_dict(),\n",
    "                'data_type_distribution': {str(k): v for k, v in dtype_counts.items()},\n",
    "                'numeric_column_quality': numeric_quality\n",
    "            }\n",
    "        \n",
    "        return quality_report\n",
    "    \n",
    "    def _count_outliers_iqr(self, series):\n",
    "        \"\"\"Count outliers using IQR method\"\"\"\n",
    "        Q1 = series.quantile(0.25)\n",
    "        Q3 = series.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        return ((series < lower_bound) | (series > upper_bound)).sum()\n",
    "\n",
    "# Initialize advanced analysis\n",
    "if 'processor' in locals():\n",
    "    print(\"\\\\n=== Initializing Advanced Analysis ===\")\n",
    "    advanced_analysis = AdvancedProteinAnalysis(processor)\n",
    "    \n",
    "    # Run large-scale feature analysis\n",
    "    feature_results = advanced_analysis.large_scale_feature_analysis()\n",
    "    print(f\"Feature analysis completed for {len(feature_results)} datasets\")\n",
    "    \n",
    "    # Run data quality analysis\n",
    "    quality_report = advanced_analysis.generate_data_quality_report()\n",
    "    print(f\"Data quality analysis completed for {len(quality_report)} datasets\")\n",
    "    \n",
    "    # Example clustering analysis (if data is available)\n",
    "    available_datasets = list(processor.csv_files.keys())\n",
    "    if available_datasets:\n",
    "        example_dataset = available_datasets[0]\n",
    "        print(f\"\\\\nRunning clustering analysis on {example_dataset}...\")\n",
    "        clustering_result = advanced_analysis.gpu_accelerated_clustering_analysis(\n",
    "            example_dataset, n_clusters=3, max_features=10\n",
    "        )\n",
    "        if clustering_result:\n",
    "            print(f\"Clustering completed: {clustering_result['n_clusters']} clusters found\")\n",
    "            for cluster_id, stats in clustering_result['cluster_statistics'].items():\n",
    "                print(f\"  {cluster_id}: {stats['size']} samples ({stats['percentage']:.1f}%)\")\n",
    "    \n",
    "else:\n",
    "    print(\"Processor not initialized. Please run the previous cell first.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
